# -*- coding: utf-8 -*-
"""New_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iBA9oExbf8bou0qnAI5gnIXGeQWo9J9v
"""

# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from wordcloud import WordCloud
from pypdf import PdfReader
import re
from collections import Counter
import difflib

from textblob import TextBlob
from textstat import textstat
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.linear_model import LinearRegression

# --------------------------------------------------
# PAGE CONFIG
# --------------------------------------------------
st.set_page_config(
    page_title="Forensic AI Master",
    page_icon="üïµÔ∏è‚Äç‚ôÇÔ∏è",
    layout="wide"
)

# --------------------------------------------------
# DATA LOADER
# --------------------------------------------------
@st.cache_data
def load_red_flag_dictionary():
    try:
        df = pd.read_csv("Annual_Report_Red_Flags.csv")
    except:
        df = pd.DataFrame({
            "Word": [
                "contingent", "estimate", "fluctuate", "litigation", "claim",
                "uncertainty", "pending", "unresolved", "material", "adverse",
                "risk", "doubt", "going concern", "restatement", "write-off",
                "impairment", "related party"
            ],
            "Category": [
                "Uncertainty", "Uncertainty", "Volatility", "Legal", "Legal",
                "Uncertainty", "Legal", "Legal", "Materiality", "Negative",
                "Risk", "Viability", "Viability", "Accounting", "Loss",
                "Loss", "Governance"
            ]
        })

    df["Word"] = df["Word"].astype(str).str.lower().str.strip()
    return df

# --------------------------------------------------
# PDF TEXT EXTRACTION
# --------------------------------------------------
@st.cache_data
def extract_text_fast(file, start_page=1, end_page=50):
    reader = PdfReader(file)
    text = ""

    total_pages = len(reader.pages)
    end_page = min(end_page, total_pages)

    bar = st.progress(0)
    for i in range(start_page - 1, end_page):
        try:
            page_text = reader.pages[i].extract_text()
            if page_text:
                text += page_text + "\n"
        except:
            pass

        bar.progress(int(((i - start_page + 1) / (end_page - start_page + 1)) * 100))

    bar.empty()
    return text

# --------------------------------------------------
# METRICS ENGINE
# --------------------------------------------------
def analyze_metrics(text, red_flag_df):
    words = re.findall(r"\w+", text.lower())
    total_words = len(words) or 1

    fog = textstat.gunning_fog(text)
    sentiment = TextBlob(text).sentiment.polarity

    passive_matches = re.findall(r"\b(was|were|been|being)\b\s+\w+ed\b", text.lower())
    passive_score = len(passive_matches) / total_words * 1000

    complex_count = sum(1 for w in words if textstat.syllable_count(w) >= 3)
    complex_pct = complex_count / total_words * 100

    red_set = set(red_flag_df["Word"])
    matched_words = [w for w in words if w in red_set]

    cat_map = dict(zip(red_flag_df.Word, red_flag_df.Category))
    matched_categories = [cat_map.get(w, "Unknown") for w in matched_words]

    return {
        "fog": fog,
        "sentiment": sentiment,
        "passive": passive_score,
        "complex": complex_pct,
        "total": total_words,
        "matched": matched_words,
        "cats": matched_categories
    }

def calculate_similarity(t1, t2):
    vectorizer = CountVectorizer().fit_transform([t1[:5000], t2[:5000]])
    return cosine_similarity(vectorizer)[0][1]

# --------------------------------------------------
# SIDEBAR
# --------------------------------------------------
with st.sidebar:
    st.title("üïµÔ∏è‚Äç‚ôÇÔ∏è Forensic Inputs")
    file_curr = st.file_uploader("Current Year Annual Report (PDF)", type="pdf")
    file_prev = st.file_uploader("Previous Year Report (Optional)", type="pdf")

    st.markdown("---")
    net_income = st.number_input("Net Income (Cr)", 0.0)
    cash_flow = st.number_input("Cash Flow from Operations (Cr)", 0.0)

# --------------------------------------------------
# MAIN DASHBOARD
# --------------------------------------------------
if file_curr:
    red_df = load_red_flag_dictionary()

    text_curr = extract_text_fast(file_curr)
    metrics = analyze_metrics(text_curr, red_df)

    similarity = None
    if file_prev:
        text_prev = extract_text_fast(file_prev)
        similarity = calculate_similarity(text_curr, text_prev)

    st.title("üìä Forensic Analysis Dashboard")
    st.caption(f"Total Words Analyzed: {metrics['total']:,}")

    if cash_flow > 0 and net_income > cash_flow * 1.5:
        st.warning("‚ö†Ô∏è **Accruals Warning:** Net income significantly exceeds operating cash flow.")

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Fog Index", f"{metrics['fog']:.2f}")
    col2.metric("Sentiment", f"{metrics['sentiment']:.2f}")
    col3.metric("Passive Voice / 1k", f"{metrics['passive']:.1f}")
    col4.metric("YoY Similarity", f"{similarity*100:.1f}%" if similarity else "N/A")

    st.markdown("---")

    # Red Flag Word Cloud
    if metrics["matched"]:
        st.subheader("üö© Red Flag Word Cloud")
        wc = WordCloud(
            background_color="white",
            colormap="Reds",
            width=700,
            height=350
        ).generate(" ".join(metrics["matched"]))

        fig, ax = plt.subplots()
        ax.imshow(wc)
        ax.axis("off")
        st.pyplot(fig)
    else:
        st.success("‚úÖ No red-flag dictionary words detected.")

    # --------------------------------------------------
    # QUALITATIVE FORENSICS
    # --------------------------------------------------
    st.markdown("---")
    st.subheader("üß† Qualitative Linguistic Analysis")

    class QualitativeForensics:
        def __init__(self, text):
            self.text = text.lower()

        def analyze_justification_density(self):
            triggers = [
                "because", "due to", "owing to", "as a result of", "thereby",
                "in order to", "despite", "although", "attributed to", "driven by"
            ]
            total_words = len(self.text.split()) or 1
            count = sum(self.text.count(t) for t in triggers)
            score = count / total_words * 1000

            if score > 15:
                level = "HIGH"
            elif score > 8:
                level = "MEDIUM"
            else:
                level = "LOW"

            return level, score

        def detect_boilerplate(self, prev_text):
            ratio = difflib.SequenceMatcher(
                None, self.text[:5000], prev_text[:5000]
            ).ratio()
            return ratio * 100

    ql = QualitativeForensics(text_curr)
    level, score = ql.analyze_justification_density()
    st.metric("Justification Density", f"{score:.2f}", level)

    if file_prev:
        boilerplate = ql.detect_boilerplate(text_prev)
        st.metric("Boilerplate Similarity", f"{boilerplate:.1f}%")

else:
    st.info("‚¨ÖÔ∏è Upload a PDF to start forensic analysis.")